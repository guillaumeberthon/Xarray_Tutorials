{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\" \n",
    "     width=\"30%\" \n",
    "     align=right\n",
    "     alt=\"Dask logo\">\n",
    "     \n",
    "# Introduction to Dask \n",
    "\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "\n",
    "### Overview\n",
    "    \n",
    "* **teaching:** 20 minutes\n",
    "* **exercises:** 0\n",
    "* **questions:**\n",
    "    * How does Dask parallelize computations in Python?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [**What-is-Dask?**](#What-is-Dask?)\n",
    "1. [**Dask Collections**](#Dask-Collections)\n",
    "1. [**Parallelism using the dask.distributed scheduler**](#Parallelism-using-the-dask.distributed-scheduler)\n",
    "1. [**Profiling & Diagnostics using the Dask Dashboard**](#Profiling-&-Diagnostics-using-the-Dask-Dashboard)\n",
    "1. [**Distributed Dask clusters for HPC and Cloud environments**](#Distributed-Dask-clusters-for-HPC-and-Cloud-environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dask?\n",
    "\n",
    "Dask is a flexible parallel computing library for analytic computing. Dask provides dynamic parallel task scheduling and high-level big-data collections like `dask.array` and `dask.dataframe`, and an extensive suite of deployment options. Dask's documentation can be found here: https://docs.dask.org/en/latest/\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-overview.svg\" \n",
    "     width=\"75%\" \n",
    "     align=center\n",
    "     alt=\"Dask overview\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick setup\n",
    "\n",
    "For the purposes of this notebook, we'll use a Dask Cluster to manage computations. The next cell sets up a simple LocalCluster. We'll cover Dask schedulers and clusters later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&#128070</p> Click the Dashboard link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Collections\n",
    "\n",
    "Dask includes 3 main collections:\n",
    "\n",
    "- [Dask Array](https://docs.dask.org/en/latest/array.html): Parallel NumPy arrays\n",
    "- [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html): Parallel Pandas DataFrames\n",
    "- [Dask Bag](https://docs.dask.org/en/latest/bag.html): Parallel Python Lists\n",
    "\n",
    "Xarray primarily interfaces with the Dask Array collection so we'll skip the others for now. You can find out more about Dask's user interfaces [here](https://docs.dask.org/en/latest/user-interfaces.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Arrays\n",
    "\n",
    "Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using multiple cores. We coordinate these blocked algorithms using Dask graphs. Dask Array's are also _lazy_, meaning that they do not evaluate until you explicitly ask for a result using the `compute` method.\n",
    "\n",
    "If we want to create a NumPy array of all ones, we do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shape = (1000, 4000)\n",
    "ones_np = np.ones(shape)\n",
    "ones_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array contains exactly 32 MB of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.1f MB' % (ones_np.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the same array using Dask's array interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "ones = da.ones(shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but we didn't tell Dask how to split up (or chunk) the array, so it is not optimized for parallel computation.\n",
    "\n",
    "A crucal difference with Dask is that we must specify the `chunks` argument. \"Chunks\" describes how the array is split up over many sub-arrays.\n",
    "\n",
    "![Dask Arrays](http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg)\n",
    "_source: [Dask Array Documentation](http://dask.pydata.org/en/latest/array-overview.html)_\n",
    "\n",
    "There are [several ways to specify chunks](http://dask.pydata.org/en/latest/array-creation.html#chunks).\n",
    "In this lecture, we will use a block shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_shape = (1000, 1000)\n",
    "ones = da.ones(shape, chunks=chunk_shape)\n",
    "ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we just see a symbolic represetnation of the array, including its shape, dtype, and chunksize.\n",
    "No data has been generated yet.\n",
    "When we call `.compute()` on a Dask array, the computation is trigger and the dask array becomes a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happened when we called `.compute()`, we can visualize the Dask _graph_, the symbolic operations that make up the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our array has four chunks. To generate it, Dask calls `np.ones` four times and then concatenates this together into one array.\n",
    "\n",
    "Rather than immediately loading a Dask array (which puts all the data into RAM), it is more common to reduce the data somehow. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_ones = ones.sum()\n",
    "sum_of_ones.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Dask's strategy for finding the sum. This simple example illustrates the beauty of Dask: it automatically designs an algorithm appropriate for custom operations with big data. \n",
    "\n",
    "If we make our operation more complex, the graph gets more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_calculation = (ones * ones[::-1, ::-1]).mean()\n",
    "fancy_calculation.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bigger Calculation\n",
    "\n",
    "The examples above were toy examples; the data (32 MB) is probably not big enough to warrant the use of Dask.\n",
    "\n",
    "We can make it a lot bigger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigshape = (200000, 4000)\n",
    "big_ones = da.ones(bigshape, chunks=chunk_shape)\n",
    "big_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.1f MB' % (big_ones.nbytes / 1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is 6.4 GB, rather than 32 MB! This is probably close to or greater than the amount of available RAM than you have in your computer. Nevertheless, Dask has no problem working on it.\n",
    "\n",
    "_Do not try to `.visualize()` this array!_\n",
    "\n",
    "When doing a big calculation, dask also has some tools to help us understand what is happening under the hood. Let's watch the dashboard again as we do a bigger computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_calc = (big_ones * big_ones[::-1, ::-1]).mean()\n",
    "\n",
    "result = big_calc.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction \n",
    "\n",
    "All the usual numpy methods work on dask arrays.\n",
    "You can also apply numpy function directly to a dask array, and it will stay lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ones_reduce = (np.cos(big_ones)**2).mean(axis=1)\n",
    "big_ones_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting also triggers computation, since we need the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(big_ones_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism using the dask.distributed scheduler\n",
    "\n",
    "In the [first cell](#Quick-setup) of this notebook, we started a local Dask Cluster and Client. We skipped past some important details there that we'll unpack now.\n",
    "\n",
    "### Dask Schedulers\n",
    "\n",
    "The Dask *Schedulers* orchestrate the tasks in the Task Graphs so that they can be run in parallel.  *How* they run in parallel, though, is determined by which *Scheduler* you choose.\n",
    "\n",
    "There are 3 *local* schedulers:\n",
    "\n",
    "- **Single-Thread Local:** For debugging, profiling, and diagnosing issues\n",
    "- **Multi-threaded:** Using the Python built-in `threading` package (the default for all Dask operations except `Bags`)\n",
    "- **Multi-process:** Using the Python built-in `multiprocessing` package (the default for Dask `Bags`)\n",
    "\n",
    "and 1 *distributed* scheduler, which we will talk about later:\n",
    "\n",
    "- **Distributed:** Using the `dask.distributed` module (which uses `tornado` for communication over TCP). The distributed scheduler uses a `Cluster` to manage communication between the scheduler and the \"workers\". This is described in the next section.\n",
    "\n",
    "### Distributed Clusters (http://distributed.dask.org/)\n",
    "\n",
    "\n",
    "- `LocalCluster` - Creates a `Cluster` that can be executed locally. Each `Cluster` includes a `Scheduler` and `Worker`s. \n",
    "- `Client` - Connects to and drives computation on a distributed `Cluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling & Diagnostics using the Dask Dashboard\n",
    "\n",
    "You'll recall from above, that we opened a url to the Dask Dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard the Dask distributed scheduler provides a an incredibly valuable tool for gaining insights into the performance of your computation and the cluster as a whole. In the dashboard, you'll see a number of tags:\n",
    "\n",
    "- _Status_: Overview of the current state of the scheduler, including the active task stream, progress, memory per worker, and the number of tasks per worker.\n",
    "- _Workers_: The workers tab allows you to track cpu and memory use per worker.\n",
    "- _System_: Live tracking of system resources like cpu, memory, bandwidth, and open file descriptors\n",
    "- _Profile_: Fine-grained statistical profiling \n",
    "- _Info_: Worker status and logs.\n",
    "\n",
    "Another useful diagnostic tool is Dask's static performance report. This allows you to save a report, including the task stream, worker profiles, etc. for all or a specific part of a workflow. Below is an example of how you would create such a report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import performance_report\n",
    "\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    big_calc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Dask clusters for HPC and Cloud environments\n",
    "\n",
    "Dask can be deployed on distributed infrastructure, such as a an HPC system or a cloud computing system. There is a growing ecosystem of Dask deployment projects that faciliate easy deployment and scaling of Dask clusters on a wide variety of computing systems.\n",
    "\n",
    "### HPC\n",
    "\n",
    "#### Dask Jobqueue (https://jobqueue.dask.org/)\n",
    "\n",
    "- `dask_jobqueue.PBSCluster`\n",
    "- `dask_jobqueue.SlurmCluster`\n",
    "- `dask_jobqueue.LSFCluster`\n",
    "- etc.\n",
    "\n",
    "#### Dask MPI (https://mpi.dask.org/)\n",
    "\n",
    "- `dask_mpi.initialize`\n",
    "\n",
    "### Cloud\n",
    "\n",
    "#### Dask Kubernetes (https://kubernetes.dask.org/)\n",
    "\n",
    "- `dask_kubernetes.KubeCluster`\n",
    "\n",
    "#### Dask Cloud Provider (https://cloudprovider.dask.org)\n",
    "\n",
    "- `dask_cloudprovider.FargateCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "- `dask_cloudprovider.ECSCluster`\n",
    "\n",
    "#### Dask Gateway (https://gateway.dask.org/)\n",
    "\n",
    "- `dask_gateway.GatewayCluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "_Note: Pieces of this notebook comes from the following sources:_\n",
    "\n",
    "- https://github.com/pangeo-data/pangeo-tutorial\n",
    "- https://github.com/rabernat/research_computing\n",
    "- https://github.com/dask/dask-examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
